{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToMe Visualization\n",
    "We provide some visualization functions to visualize the effect of ToMe like in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vim.models_mamba import vim_small_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2\n",
    "from tome.patch.mamba import apply_patch\n",
    "import torch\n",
    "import tome\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from timm.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "nb_classes = 1000\n",
    "input_size=224\n",
    "model = \"vim_small_patch16_stride8_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2\"\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the timm augreg models here, but you can use any supported implementation.\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "        model,\n",
    "        pretrained=False,\n",
    "        num_classes=nb_classes,\n",
    "        drop_rate=0.0,\n",
    "        drop_path_rate=None,\n",
    "        drop_block_rate=None,\n",
    "        img_size=input_size\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_path = \"/home/albert/ml/Mambas/vim/Vim/vim/ckpt/vim_s_midclstok_ft_81p6acc.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "# Source tracing is necessary for visualization!\n",
    "apply_patch(model, trace_source=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 224\n",
    "\n",
    "# # Make sure the transform is correct for your model!\n",
    "# transform_list = [\n",
    "#     transforms.Resize(int((256 / 224) * input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "#     transforms.CenterCrop(input_size)\n",
    "# ]\n",
    "\n",
    "# # The visualization and model need different transforms\n",
    "# transform_vis  = transforms.Compose(transform_list)\n",
    "# transform_norm = transforms.Compose(transform_list + [\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(model.default_cfg[\"mean\"], model.default_cfg[\"std\"]),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_original = Image.open(\"/home/albert/ml/Mambas/ImageNet1k/ILSVRC/Data/CLS-LOC/train/n01484850/n01484850_10086.JPEG\")\n",
    "\n",
    "\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "eval_crop_ratio = 0.875\n",
    "\n",
    "\n",
    "transforms_all = []\n",
    "transforms_vis = []\n",
    "resize_im = True\n",
    "for t in [transforms_all, transforms_vis]:\n",
    "    if resize_im:\n",
    "        size = int(input_size / eval_crop_ratio)\n",
    "        t.append(\n",
    "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "transforms_all.append(transforms.ToTensor())\n",
    "transforms_all.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
    "\n",
    "pil_transforms = transforms.Compose(transforms_vis)\n",
    "all_transforms = transforms.Compose(transforms_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil_transforms = pil_transforms(img_original)\n",
    "img_pil_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize one image\n",
    "You can play with the `r` value to get different numbers of tokens at the end of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_tensor = all_transforms(img_original)\n",
    "img_batch = img_tensor[None, :,:,:] # F.interpolate(img_tensor[None, :,:,:], 224)\n",
    "# print(img_batch.size())\n",
    "img_batch =img_batch.to(device)\n",
    "# print(img_batch.size())\n",
    "\n",
    "_ = model(img_batch)\n",
    "source = model._tome_info[\"source\"]\n",
    "\n",
    "print(f\"{source.shape[1]} tokens at the end\")\n",
    "tome.make_visualization_mamba(img_pil_transforms, source, class_token=True,token_per_dim =27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.make_visualization_mamba_only_merged_tokens(img_pil_transforms, source, class_token=True,token_per_dim =27)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize intermediate results\n",
    "Setting `r` to be a list can let us control where we stop merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.r = [25] * 8  # 8 / 24 layers\n",
    "_ = model(img_batch)\n",
    "source = model._tome_info[\"source\"]\n",
    "\n",
    "print(f\"{source.shape[1]} tokens at the end\")\n",
    "tome.make_visualization_mamba(img_pil_transforms, source, class_token=True,token_per_dim =27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.r = [25] * 16  # 16 / 24 layers\n",
    "_ = model(img[None, ...])\n",
    "source = model._tome_info[\"source\"]\n",
    "\n",
    "print(f\"{source.shape[1]} tokens at the end\")\n",
    "tome.make_visualization(img, source, patch_size=16, class_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.r = [25] * 22  # 22 / 24 layers\n",
    "_ = model(img[None, ...])\n",
    "source = model._tome_info[\"source\"]\n",
    "\n",
    "print(f\"{source.shape[1]} tokens at the end\")\n",
    "tome.make_visualization(img, source, patch_size=16, class_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vimAndMamba] *",
   "language": "python",
   "name": "conda-env-vimAndMamba-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "05c3e7d5f676d27e80c59167888c7f79621c476ae5272fc4e435fcce0be043ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
